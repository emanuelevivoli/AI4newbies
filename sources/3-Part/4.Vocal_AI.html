
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>4. Vocal AI &#8212; chatGPT wrote this BOOK</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="5. Natural Language Processing" href="5.Natural_Language_Processing.html" />
    <link rel="prev" title="3. Computer Vision" href="3.Computer_Vision.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.jpeg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">chatGPT wrote this BOOK</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    What is this book?
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Preliminaries Notions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1-Part/preliminaries.html">
   AI4noobs - Preliminaries
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1-Part/1.Programming_with_Notebooks.html">
     1. Programming with Notebooks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1-Part/2.Introduction_to_Programming.html">
     2. Introduction to Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1-Part/3.Data_Structures.html">
     3. Data Structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1-Part/4.Computational_Complexity.html">
     4. Computational Complexity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1-Part/5.Statistics.html">
     5. Statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1-Part/6.Linear_Algebra.html">
     6. Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1-Part/7.Geometry.html">
     7. Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1-Part/8.Calculus.html">
     8. Numerical Calculus
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basis of AI
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2-Part/basis.html">
   AI4noobs - Basis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2-Part/1.Introduction_to_Artificial_Intelligence_and_Deep_Learning.html">
     1. Introduction to Artificial Intelligence and Deep Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2-Part/2.The_Basics_of_Machine_Learning.html">
     2. The Basics of Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2-Part/3.Neural_Networks_and_Deep_Learning.html">
     3. Neural Networks and Deep Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2-Part/4.Natural_Language_Processing.html">
     4. Natural Language Processing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2-Part/5.Computer_Vision.html">
     5. Computer Vision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2-Part/6.Ethics_and_the_Future_of_AI.html">
     6: Ethics and the Future of AI
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Applications of AI
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="applications.html">
   AI4noobs - Applications
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="1.Fraud_Detection.html">
     1. Fraud Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2.Customer_Relationship_Management_Systems.html">
     2. Customer Relationship Management Systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.Computer_Vision.html">
     3. Computer Vision
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     4. Vocal AI
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5.Natural_Language_Processing.html">
     5. Natural Language Processing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="6.Autonomous_Vehicles.html">
     6. Autonomous Vehicles
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="7.Supercomputers.html">
     7. Supercomputers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8.Investment_Modeling.html">
     8. Investment Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9.E-commerce.html">
     9. E-commerce
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/sources/3-Part/4.Vocal_AI.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fsources/3-Part/4.Vocal_AI.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/sources/3-Part/4.Vocal_AI.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-to-vocal-ai">
   4.1 Introduction to vocal AI
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speech-recognition">
   4.2 Speech recognition
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-using-google">
     example using Google
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-using-pytorch">
     example using pytorch
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-using-pytorch-and-custom-model">
     example using pytorch and custom model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#case-study-virtual-assistants">
   4.3 Case study: Virtual assistants
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>4. Vocal AI</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-to-vocal-ai">
   4.1 Introduction to vocal AI
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speech-recognition">
   4.2 Speech recognition
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-using-google">
     example using Google
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-using-pytorch">
     example using pytorch
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-using-pytorch-and-custom-model">
     example using pytorch and custom model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#case-study-virtual-assistants">
   4.3 Case study: Virtual assistants
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="vocal-ai">
<h1>4. Vocal AI<a class="headerlink" href="#vocal-ai" title="Permalink to this headline">#</a></h1>
<section id="introduction-to-vocal-ai">
<h2>4.1 Introduction to vocal AI<a class="headerlink" href="#introduction-to-vocal-ai" title="Permalink to this headline">#</a></h2>
<p>Vocal AI, or artificial intelligence for speech and voice, is a field of AI that focuses on the development of computer systems that can understand, interpret, and generate human speech and voice. Vocal AI technologies have a wide range of applications, including speech recognition, natural language processing, voice assistants, and speech synthesis. These technologies have the potential to revolutionize how we communicate and interact with computers, making it easier and more natural to access information and perform tasks.</p>
<p>One of the key challenges in developing vocal AI technologies is the complexity and variability of human speech and voice. Human speech is often ambiguous, context-dependent, and influenced by factors such as accent, dialect, and emotion. In order to accurately understand and generate human speech, vocal AI systems must be able to handle these complexities and variations.</p>
<p>There are several approaches to developing vocal AI technologies, including rule-based systems, statistical models, and deep learning methods. Each approach has its own strengths and limitations, and the most effective vocal AI systems often combine multiple approaches.</p>
<p>In this chapter, we will explore the basics of vocal AI and the key technologies and applications in this field. We will also look at some of the challenges and opportunities in the development of vocal AI, and discuss the potential impact of these technologies on society and industry.</p>
</section>
<section id="speech-recognition">
<h2>4.2 Speech recognition<a class="headerlink" href="#speech-recognition" title="Permalink to this headline">#</a></h2>
<p>In speech recognition, the goal is to convert spoken language into text. This can be used in a variety of applications, such as dictation, voice-to-text messaging, and voice commands for devices. There are several approaches to speech recognition, including:</p>
<ol class="simple">
<li><p>Rule-based: This approach involves using a set of predefined rules and heuristics to recognize speech.</p></li>
<li><p>Connectionist: This approach involves using artificial neural networks to recognize speech.</p></li>
<li><p>HMM (hidden Markov model): This approach involves using statistical models to recognize speech.</p></li>
</ol>
<section id="example-using-google">
<h3>example using Google<a class="headerlink" href="#example-using-google" title="Permalink to this headline">#</a></h3>
<p>To implement speech recognition in Python, you can use libraries such as <strong><code class="docutils literal notranslate"><span class="pre">SpeechRecognition</span></code></strong> or <strong><code class="docutils literal notranslate"><span class="pre">pocketsphinx</span></code></strong>. Here is an example of how to use the <strong><code class="docutils literal notranslate"><span class="pre">SpeechRecognition</span></code></strong> library to recognize speech from a microphone:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">speech_recognition</span> <span class="k">as</span> <span class="nn">sr</span>

<span class="c1"># create a recognizer object</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">sr</span><span class="o">.</span><span class="n">Recognizer</span><span class="p">()</span>

<span class="c1"># start listening to the microphone</span>
<span class="k">with</span> <span class="n">sr</span><span class="o">.</span><span class="n">Microphone</span><span class="p">()</span> <span class="k">as</span> <span class="n">source</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Say something:&quot;</span><span class="p">)</span>
    <span class="n">audio</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">listen</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>

<span class="c1"># recognize the speech</span>
<span class="k">try</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;You said: &quot;</span> <span class="o">+</span> <span class="n">r</span><span class="o">.</span><span class="n">recognize_google</span><span class="p">(</span><span class="n">audio</span><span class="p">))</span>
<span class="k">except</span> <span class="n">sr</span><span class="o">.</span><span class="n">UnknownValueError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Could not understand audio&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="n">sr</span><span class="o">.</span><span class="n">RequestError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error making request: </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="n">yw</span><span class="o">/</span><span class="mi">39</span><span class="n">qgg96x2451prd0f9qrtbzr0000gn</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_8230</span><span class="o">/</span><span class="mf">3467448874.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">speech_recognition</span> <span class="k">as</span> <span class="nn">sr</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> 
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="c1"># create a recognizer object</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">r</span> <span class="o">=</span> <span class="n">sr</span><span class="o">.</span><span class="n">Recognizer</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> 

<span class="ne">ModuleNotFoundError</span>: No module named &#39;speech_recognition&#39;
</pre></div>
</div>
</div>
</div>
<p>This code will listen to the microphone, and when you speak it will try to recognize the speech and print it out. Note that this example uses the Google Speech Recognition API, which requires an internet connection.</p>
</section>
<section id="example-using-pytorch">
<h3>example using pytorch<a class="headerlink" href="#example-using-pytorch" title="Permalink to this headline">#</a></h3>
<p>Here is an example of how to perform speech recognition using PyTorch:</p>
<p>First, we will need to install the <strong><code class="docutils literal notranslate"><span class="pre">torchaudio</span></code></strong> library, which provides functionality for loading and processing audio data in PyTorch. We can install it using <strong><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">torchaudio</span></code></strong>.</p>
<p>Next, we will need to download a pre-trained speech recognition model. For this example, we will use the <strong><code class="docutils literal notranslate"><span class="pre">DeepSpeech2</span></code></strong> model, which can be downloaded from the <strong><code class="docutils literal notranslate"><span class="pre">torchaudio</span></code></strong> website or by running the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget https://download.pytorch.org/models/deepspeech2-9f5f8983.pth
</pre></div>
</div>
<p>Now we can load the model and use it to transcribe an audio file. Here is some example code that does this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchaudio</span>

<span class="c1"># Load the pre-trained model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchaudio</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">DeepSpeech2</span><span class="p">(</span><span class="n">rnn_hidden_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;deepspeech2-9f5f8983.pth&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Load the audio file and pre-process it</span>
<span class="n">waveform</span><span class="p">,</span> <span class="n">sample_rate</span> <span class="o">=</span> <span class="n">torchaudio</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;audio.wav&#39;</span><span class="p">)</span>
<span class="n">waveform</span> <span class="o">=</span> <span class="n">waveform</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># remove the dummy batch dimension</span>

<span class="c1"># Transcribe the audio</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">waveform</span><span class="p">)</span>

<span class="c1"># Convert the output to text</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">chr</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">text</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This code loads the <strong><code class="docutils literal notranslate"><span class="pre">DeepSpeech2</span></code></strong> model, loads an audio file, and transcribes it using the model. The output is a list of character indices, which we convert to a string of characters using a list comprehension. The resulting string is the transcribed text.</p>
</section>
<section id="example-using-pytorch-and-custom-model">
<h3>example using pytorch and custom model<a class="headerlink" href="#example-using-pytorch-and-custom-model" title="Permalink to this headline">#</a></h3>
<p>Here is an example of using PyTorch for speech recognition:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># Define a simple convolutional neural network</span>
<span class="k">class</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc5</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Load the dataset and preprocess it</span>
<span class="kn">import</span> <span class="nn">torchaudio</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">torchaudio</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">VCTK</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;path/to/VCTK/dir&#39;</span><span class="p">)</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Define the loss function and optimizer</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>This example trains a simple convolutional neural network on the VCTK dataset for speech recognition. The dataset is loaded using the <strong><code class="docutils literal notranslate"><span class="pre">torchaudio</span></code></strong> library, which handles all the preprocessing steps such as filtering and normalization. The model is trained using the Adam optimizer and the cross-entropy loss function.</p>
</section>
</section>
<section id="case-study-virtual-assistants">
<h2>4.3 Case study: Virtual assistants<a class="headerlink" href="#case-study-virtual-assistants" title="Permalink to this headline">#</a></h2>
<p>Virtual assistants are a popular application of vocal AI, and they have become an integral part of many people’s daily lives. These assistants, such as Apple’s Siri or Amazon’s Alexa, are able to understand and respond to voice commands and questions.</p>
<p>To build a virtual assistant, you need to first develop a speech recognition system that can understand and transcribe spoken words. This can be done using a variety of techniques, including hidden Markov models, Gaussian mixture models, and deep learning approaches.</p>
<p>Once you have a speech recognition system in place, you can use natural language processing (NLP) techniques to understand the meaning of the words and generate appropriate responses. NLP is a field of artificial intelligence that deals with the interaction between computers and humans through the use of natural language.</p>
<p>Here is an example of how you can use a pre-trained model from Hugging Face’s <strong><code class="docutils literal notranslate"><span class="pre">transformers</span></code></strong> library to build a virtual assistant in PyTorch:</p>
<p>First, let’s install the required libraries:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install transformers
<span class="o">!</span>pip install torch
</pre></div>
</div>
</div>
</div>
<p>Next, we can use the <strong><code class="docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></strong> class from the <strong><code class="docutils literal notranslate"><span class="pre">transformers</span></code></strong> library to load a pre-trained language model. This model has been trained to generate human-like text, so it can be used to generate responses for our virtual assistant:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>

<span class="c1"># Load a pre-trained language model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="c1"># Set the model to evaluation mode</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have our pre-trained model loaded, we can use it to generate responses to user input. Here is a simple function that takes a user’s input and returns a response generated by the model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">generate_response</span><span class="p">(</span><span class="n">input_text</span><span class="p">):</span>
    <span class="c1"># Encode the input text and add the special tokens</span>
    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

    <span class="c1"># Generate a response</span>
    <span class="n">response_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>

    <span class="c1"># Decode the response and remove the special tokens</span>
    <span class="n">response_text</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">response_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">response_text</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we can use our virtual assistant by calling the <strong><code class="docutils literal notranslate"><span class="pre">generate_response</span></code></strong> function with user input:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">&quot;Hello, how are you?&quot;</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">generate_response</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="c1"># Output: I&#39;m doing well, thanks for asking! How about you?</span>
</pre></div>
</div>
</div>
</div>
<p>This is just a simple example of how you can use a pre-trained language model to build a virtual assistant in PyTorch. You can experiment with different models and customize the virtual assistant to suit your needs.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./sources/3-Part"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="3.Computer_Vision.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">3. Computer Vision</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="5.Natural_Language_Processing.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">5. Natural Language Processing</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Book about AI/ML/DL (vomited by ChatGPT)<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>