{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Networks and Deep Learning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3.1 **Introduction**\n",
    "\n",
    "> Definition of neural networks: a type of machine learning algorithm inspired by the structure and function of the brain\n",
    ">\n",
    "> Key components of a neural network: input layer, hidden layer(s), output layer, and weights\n",
    ">\n",
    "> Activation functions: sigmoid, tanh, ReLU, etc.\n",
    ">\n",
    "> Backpropagation: an algorithm for training neural networks by adjusting the weights to minimize the error between the predicted and true outputs\n",
    "> \n",
    "\n",
    "Neural networks are a type of machine learning algorithm inspired by the structure and function of the brain. They are composed of a series of interconnected \"neurons\" that can process and transmit information.\n",
    "\n",
    "A neural network consists of an input layer, one or more hidden layers, and an output layer. Each layer consists of a set of \"neurons,\" which are connected to the neurons in the next layer via a set of weights. The input layer receives the input data, and the output layer produces the final prediction or classification. The hidden layers are responsible for extracting features and patterns from the input data and passing them on to the output layer.\n",
    "\n",
    "The output of a neuron is computed using the following formula:\n",
    "\n",
    "$$\n",
    "y = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b)\n",
    "$$\n",
    "\n",
    "where $y$ is the output, $f$ is the activation function, $x_1, x_2, ..., x_n$ are the inputs, $w_1, w_2, ..., w_n$ are the weights, and $b$ is the bias term.\n",
    "\n",
    "Activation functions are used to introduce nonlinearity into the network. Common activation functions include the sigmoid function:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "the tanh function:\n",
    "\n",
    "$$\n",
    "tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "and the rectified linear unit (ReLU) function:\n",
    "\n",
    "$$\n",
    "ReLU(x) = max(0, x)\n",
    "$$\n",
    "\n",
    "Backpropagation is an algorithm for training neural networks by adjusting the weights to minimize the error between the predicted and true outputs. It involves performing a forward pass through the network to compute the output, and then a backward pass through the network to compute the gradients of the weights with respect to the error.\n",
    "\n",
    "The gradients are then used to update the weights using an optimization algorithm, such as stochastic gradient descent. The update rule for the weights is:\n",
    "\n",
    "$$\n",
    "w_i = w_i - \\alpha \\frac{\\partial E}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "where $w_i$ is the weight, $\\alpha$ is the learning rate, and $\\frac{\\partial E}{\\partial w_i}$ is the gradient of the error with respect to the weight.\n",
    "\n",
    "In the following sections, we will explore various types of neural networks, including single layer perceptrons, multilayer perceptrons, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). We will also discuss the concept of deep learning, which involves building very large and complex neural networks for tasks such as natural language processing, computer vision, and speech recognition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 3.2 **Single layer perceptron**\n",
    "\n",
    "> Definition: a type of neural network with a single layer of weights\n",
    ">\n",
    "> Key formula: the perceptron learning rule, which updates the weights based on the error between the predicted and true outputs\n",
    "> \n",
    "\n",
    "A single layer perceptron is a type of neural network with a single layer of weights. It is a simple model that is often used as a baseline for more complex models.\n",
    "\n",
    "The output of a single layer perceptron is computed using the following formula:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases} 1 & \\text{if } w_1x_1 + w_2x_2 + ... + w_nx_n + b > 0 \\\\ 0 &\\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "where $y$ is the output, $x_1, x_2, ..., x_n$ are the inputs, $w_1, w_2, ..., w_n$ are the weights, and $b$ is the bias term.\n",
    "\n",
    "The single layer perceptron can be trained using the perceptron learning rule, which updates the weights based on the error between the predicted and true outputs. The update rule is:\n",
    "\n",
    "$$\n",
    "w_i = w_i + \\alpha (y_{true} - y_{pred}) x_i\n",
    "$$\n",
    "\n",
    "where $w_i$ is the weight, $\\alpha$ is the learning rate, $y_{true}$ is the true output, $y_{pred}$ is the predicted output, and $x_i$ is the input.\n",
    "\n",
    "The single layer perceptron is a linear model, which means that it can only learn to separate the data using a linear boundary. This limits its ability to model more complex patterns in the data. However, it can still be a useful tool for simple classification tasks.\n",
    "\n",
    "One of the main drawbacks of the single layer perceptron is that it is prone to getting stuck in local minima, meaning that it may not find the optimal solution to the problem. This can be mitigated by using a more sophisticated optimization algorithm, such as stochastic gradient descent.\n",
    "\n",
    "It is also worth noting that the single layer perceptron is a binary classifier, meaning that it can only predict one of two classes. To classify data into more than two classes, a multi-class classifier such as a multilayer perceptron or a support vector machine (SVM) may be more appropriate.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3.3 **Multilayer perceptron**\n",
    "\n",
    "> Definition: a type of neural network with multiple layers of weights\n",
    ">\n",
    "> Key formulas: the forward pass, which computes the output of the neural network given an input, and the backward pass, which updates the weights using backpropagation\n",
    "> \n",
    "\n",
    "A multilayer perceptron (MLP) is a type of neural network with multiple layers of weights. It is a more powerful model than a single layer perceptron, as it can learn to model more complex patterns in the data.\n",
    "\n",
    "The output of an MLP is computed using the following formula:\n",
    "\n",
    "$$\n",
    "y = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b)\n",
    "$$\n",
    "\n",
    "where $y$ is the output, $f$ is the activation function, $x_1, x_2, ..., x_n$ are the inputs, $w_1, w_2, ..., w_n$ are the weights, and $b$ is the bias term.\n",
    "\n",
    "The weights of an MLP are trained using backpropagation, an algorithm for adjusting the weights to minimize the error between the predicted and true outputs. The process involves performing a forward pass through the network to compute the output, and then a backward pass through the network to compute the gradients of the weights with respect to the error. The gradients are then used to update the weights using an optimization algorithm, such as stochastic gradient descent.\n",
    "\n",
    "MLPs are commonly used for tasks such as classification and regression. They are often used in conjunction with other techniques, such as regularization (to prevent overfitting) and early stopping (to avoid training for too long).\n",
    "\n",
    "One of the main advantages of MLPs is that they are universal function approximators, meaning that they can learn to approximate any continuous function to arbitrary accuracy given enough hidden units and data.\n",
    "\n",
    "However, they can be prone to overfitting if the number of hidden units is too large or if the training data is insufficient. To mitigate this risk, it is often helpful to use techniques such as regularization (e.g. L2 regularization) and early stopping (to stop training before the model starts to overfit).\n",
    "\n",
    "MLPs can also be slow to train, especially on large datasets, due to the need to compute and backpropagate gradients through multiple layers. This can be mitigated by using more efficient optimization algorithms (such as Adam or RProp) or by using hardware accelerators such as graphics processing units (GPUs).\n",
    "\n",
    "In summary, multilayer perceptrons are a powerful and widely-used type of neural network that can be used for a variety of tasks. They are flexible and can model complex patterns in the data, but they can be prone to overfitting and may require careful tuning to achieve good performance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 **Convolutional neural networks (CNNs)**\n",
    "\n",
    "> Definition: a type of neural network designed for image recognition tasks\n",
    ">\n",
    "> Key components: convolutional layers, pooling layers, and fully connected layers\n",
    ">\n",
    "> Key formula: the convolution operation, which extracts features from the input image using a set of filters\n",
    "> \n",
    "\n",
    "Convolutional neural networks (CNNs) are a type of neural network designed specifically for image recognition tasks. They are particularly effective at extracting features and patterns from images, and have achieved state-of-the-art results on many benchmarks.\n",
    "\n",
    "CNNs are composed of three main types of layers: convolutional layers, pooling layers, and fully connected layers.\n",
    "\n",
    "- Convolutional layers: These layers apply a set of filters to the input image, producing a feature map. The filters are learned during training and are responsible for extracting features from the image. The key formula for the convolution operation is:\n",
    "\n",
    "$$\n",
    "(f*g)(x,y) = \\sum_{u=-\\infty}^{\\infty} \\sum_{v=-\\infty}^{\\infty} f(u,v)g(x-u,y-v)\n",
    "$$\n",
    "\n",
    "where $f$ is the input image, $g$ is the filter, and $(f*g)(x,y)$ is the output feature map at position $(x,y)$.\n",
    "\n",
    "- Pooling layers: These layers downsample the feature maps produced by the convolutional layers, reducing the spatial resolution and increasing the invariance to translations. Common pooling operations include max pooling and average pooling.\n",
    "    \n",
    "    Pooling layers are used to downsample the feature maps produced by the convolutional layers, reducing the spatial resolution and increasing the invariance to translations. Common pooling operations include max pooling and average pooling.\n",
    "    \n",
    "    ### Max pooling\n",
    "    \n",
    "    Max pooling is a pooling operation that takes the maximum value from each pooling window. The output of a max pooling layer can be computed using the following formula:\n",
    "    \n",
    "    $$\n",
    "    y_{i,j} = \\max_{k=0}^{K-1} \\max_{l=0}^{L-1} x_{i+k,j+l}\n",
    "    $$\n",
    "    \n",
    "    where $y_{i,j}$ is the output at position $(i,j)$, $x$ is the input feature map, and $K$ and $L$ are the pooling window sizes.\n",
    "    \n",
    "    ### Average pooling\n",
    "    \n",
    "    Average pooling is a pooling operation that takes the average value from each pooling window. The output of an average pooling layer can be computed using the following formula:\n",
    "    \n",
    "    $$\n",
    "    y_{i,j} = \\frac{1}{KL} \\sum_{k=0}^{K-1} \\sum_{l=0}^{L-1} x_{i+k,j+l}\n",
    "    $$\n",
    "    \n",
    "    where $y_{i,j}$ is the output at position $(i,j)$, $x$ is the input feature map, and $K$ and $L$ are the pooling window sizes.\n",
    "    \n",
    "- Fully connected layers: These layers are similar to the fully connected layers in a standard multilayer perceptron. They take the flattened feature maps produced by the convolutional and pooling layers as input and output a prediction or classification.\n",
    "    \n",
    "    Fully connected layers in a CNN are similar to the fully connected layers in a standard multilayer perceptron (MLP). They take the flattened feature maps produced by the convolutional and pooling layers as input and output a prediction or classification.\n",
    "    \n",
    "    The output of a fully connected layer can be computed using the following formula:\n",
    "    \n",
    "    $$\n",
    "    y = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b)\n",
    "    $$\n",
    "    \n",
    "    where $y$ is the output, $f$ is the activation function, $x_1, x_2, ..., x_n$ are the inputs, $w_1, w_2, ..., w_n$ are the weights, and $b$ is the bias term.\n",
    "    \n",
    "    The weights of the fully connected layers are typically initialized using techniques such as Glorot initialization or He initialization, which help to prevent the \"vanishing gradient\" problem that can occur in deep networks. They are then trained using backpropagation, just like the weights in an MLP.\n",
    "    \n",
    "    Fully connected layers are often used in the final layers of a CNN to produce a prediction or classification. They can be used for tasks such as image classification, object detection, and segmentation.\n",
    "    \n",
    "    It is worth noting that fully connected layers can be computationally intensive to train, especially on large datasets. This can be mitigated by using more efficient optimization algorithms (such as Adam or RProp) or by using hardware accelerators such as graphics processing units (GPUs).\n",
    "    \n",
    "\n",
    "CNNs are trained using backpropagation, just like MLPs. However, the training process for CNNs is somewhat different due to the presence of the convolutional and pooling layers. In particular, the weights of the convolutional layers are typically initialized using techniques such as Glorot initialization or He initialization, which help to prevent the \"vanishing gradient\" problem that can occur in deep networks.\n",
    "\n",
    "CNNs are widely used in tasks such as image classification, object detection, and segmentation. They are particularly effective at recognizing patterns and features in images, and have been used to achieve state-of-the-art results on many benchmarks. However, they can be computationally intensive to train, especially on large datasets, and may require specialized hardware such as GPUs to achieve good performance.\n",
    "\n",
    "One of the key advantages of CNNs is their ability to learn hierarchical representations of the data. By using multiple layers of convolutional and pooling operations, a CNN can learn to recognize increasingly complex patterns in the data. This allows them to achieve very high accuracy on tasks such as image classification.\n",
    "\n",
    "Another advantage of CNNs is their ability to generalize well to new data. Because they learn hierarchical representations of the data, they are able to recognize patterns that are invariant to certain transformations, such as translations and rotations. This makes them robust to variations in the input data and allows them to perform well on unseen data.\n",
    "\n",
    "However, CNNs also have some limitations. One of the main limitations is their reliance on a fixed input size. Because the convolutional and pooling layers expect a fixed-size input, the input data must be resized or padded to fit the required dimensions. This can be a challenge when working with images of different sizes or when dealing with images that have been distorted or transformed in some way.\n",
    "\n",
    "In summary, convolutional neural networks are a powerful and widely-used type of neural network that are particularly effective at image recognition tasks. They are able to learn hierarchical representations of the data and are robust to variations in the input, but they are limited by their reliance on a fixed input size.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 **Recurrent neural networks (RNNs)**\n",
    "\n",
    "> Definition: a type of neural network designed for sequence modeling tasks\n",
    ">\n",
    "> Key components: recurrent layers (e.g. LSTM, GRU) and attention mechanisms\n",
    ">\n",
    "> Key formula: the attention mechanism, which allows the model to selectively focus on different parts of the input sequence\n",
    "> \n",
    "\n",
    "Recurrent neural networks (RNNs) are a type of neural network designed to process sequential data. They are particularly useful for tasks such as language translation, language modeling, and time series prediction.\n",
    "\n",
    "RNNs are composed of recurrent units, which are responsible for maintaining an internal state that can be updated based on the input data. The output of an RNN at time step $t$ is computed using the following formula:\n",
    "\n",
    "$$\n",
    "h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = g(W_{hy}h_t + b_y)\n",
    "$$\n",
    "\n",
    "where $h_t$ is the hidden state at time step $t$, $x_t$ is the input at time step $t$, $y_t$ is the output at time step $t$, $W_{hh}$ and $W_{xh}$ are the weights connecting the hidden state to itself and the input, respectively, $W_{hy}$ is the weight connecting the hidden state to the output, $b_h$ is the bias term for the hidden state, and $b_y$ is the bias term for the output. $f$ and $g$ are the activation functions for the hidden state and output, respectively.\n",
    "\n",
    "The key advantage of RNNs is their ability to capture temporal dependencies in the data. By maintaining an internal state that can be updated based on the input data, RNNs can learn to model patterns that depend on the order of the data. This makes them particularly useful for tasks such as language translation and language modeling, where the meaning of a word or phrase can depend on the words that come before and after it.\n",
    "\n",
    "RNNs are trained using backpropagation through time (BPTT), which is a variant of standard backpropagation that allows the gradients to be propagated back through multiple time steps. The gradients are computed using the chain rule, and are used to update the weights using an optimization algorithm such as stochastic gradient descent.\n",
    "\n",
    "There are several variations of RNNs, including long short-term memory (LSTM) networks and gated recurrent units (GRUs). These variations are designed to address some of the limitations of standard RNNs, such as the vanishing gradient problem (which can make it difficult to train deep RNNs) and the difficulty in learning long-term dependencies (which can make it hard for RNNs to model patterns that span many time steps).\n",
    "\n",
    "LSTM networks are a type of RNN that use a special type of recurrent unit called a \"memory cell\" to store and update the internal state. The memory cell is controlled by three \"gates\" (an input gate, an output gate, and a forget gate), which determine which information is stored in the memory cell and which information is passed on to the output. This allows LSTMs to learn long-term dependencies and to avoid the vanishing gradient problem.\n",
    "\n",
    "GRUs are another type of RNN that use a simpler type of recurrent unit called a \"gated recurrent unit.\" They are similar to LSTMs in that they use gates to control the flow of information, but they have fewer parameters and are simpler to train.\n",
    "\n",
    "In summary, RNNs are a powerful type of neural network that are particularly useful for tasks that involve sequential data. They are able to capture temporal dependencies in the data and are useful for tasks such as language translation and language modeling. There are several variations of RNNs, including LSTM networks and GRUs, which are designed to address some of the limitations of standard RNNs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 3.6 **Deep learning**\n",
    "\n",
    "> Definition: a subfield of machine learning that involves building very large and complex neural networks\n",
    ">\n",
    "> Key applications: natural language processing, computer vision, speech recognition, and more\n",
    ">\n",
    "> Key challenges: training and deploying deep learning models, avoiding overfitting, and interpretability\n",
    "> \n",
    "\n",
    "Deep learning is a subfield of machine learning that is concerned with the design and development of deep neural networks. Deep neural networks are neural networks with a large number of layers (typically more than three), and are able to learn hierarchical representations of the data. They have been successful in a wide range of tasks, including image classification, object detection, natural language processing, and speech recognition.\n",
    "\n",
    "One of the key advantages of deep learning is its ability to learn features automatically from the data. In traditional machine learning approaches, the features are often hand-engineered by the practitioner. This can be a time-consuming and error-prone process, and may not always result in the best features for the task. In contrast, deep learning algorithms are able to learn the features directly from the data, allowing them to capture complex patterns and relationships in the data. This is often achieved using multiple layers of nonlinear transformations, such as convolutions and fully connected layers.\n",
    "\n",
    "Another advantage of deep learning is its ability to scale to large datasets. Because deep neural networks are able to learn from data in an incremental fashion, they are well-suited to tasks where the amount of data is very large. This has made them particularly effective for tasks such as image classification, where large datasets of labeled images are readily available.\n",
    "\n",
    "Despite their success, deep learning algorithms are not a panacea. They can be computationally intensive to train, and may require specialized hardware (such as graphics processing units) to achieve good performance. They can also be prone to overfitting, especially when the amount of data is limited. To mitigate these issues, practitioners often use techniques such as regularization, early stopping, and data augmentation.\n",
    "\n",
    "Regularization is a technique that helps to prevent overfitting by adding a penalty to the loss function during training. Common regularization techniques include weight decay, which adds a penalty proportional to the weights of the network, and dropout, which randomly sets a portion of the activations to zero during training.\n",
    "\n",
    "Early stopping is a technique that involves stopping the training process before the model has fully converged, in order to prevent overfitting. This is often done by monitoring the performance of the model on a validation set, and stopping training when the performance on the validation set starts to deteriorate.\n",
    "\n",
    "Data augmentation is a technique that involves generating additional training data by applying transformations to the existing training data. This can help to prevent overfitting and improve the generalization of the model. Common transformations include rotating and scaling images, or adding noise to audio signals.\n",
    "\n",
    "In summary, deep learning is a subfield of machine learning concerned with the design and development of deep neural networks. Deep neural networks are able to learn hierarchical representations of the data and are able to scale to large datasets, but can be computationally intensive to train and may be prone to overfitting. Techniques such as regularization, early stopping, and data augmentation can be used to mitigate these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.4 (default, Aug 13 2019, 15:17:50) \n[Clang 4.0.1 (tags/RELEASE_401/final)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "bede37849ef1a016272327115736fc1a672222222570e1af63a91088e5ca31d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
